---
title: "Child Marriage Phenomenon In Vietnam"
author: "Holly Duong"
date: "2024-02-29"
output:
  html_document: default
  pdf_document: default
  always_allow_html: true
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
# load packages
library(readr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(plotly)
library(reshape2)
library(car)
library(kableExtra)
library(broom)
library(knitr)
library(pROC)
library(ggpattern)
library(tidyr)
library(ResourceSelection)
library(sf)
library(arm)

```

### Setting up Data

```{r message=FALSE, warning=FALSE}
# import dataset
female <- read_csv("/Users/hollyduong/Desktop/DA 401/ChildMarriageInVietnam/Data/wm.csv")
```

```{r}
# get a glimpse of dataset
head(female)
```

```{r}
# Select the specified columns to create a new dataframe
female_df <- dplyr::select(female, WAGEM, MSTATUS, HH6, HH7, welevel, insurance, ethnicity, windex5, CP2, HA1, MT4, MT9, MT11)

# View the first few rows of the new dataframe
summary(female_df)
```

```{r}
# Rename the columns
female_df <- female_df %>%
  rename(
  age_first_marriage = WAGEM,
  marital_status = MSTATUS,
  area = HH6,
  region = HH7,
  education_level = welevel,
  health_insurance = insurance,
  ethnicity = ethnicity,
  wealth_index = windex5,
  current_contraceptive_use = CP2,
  awareness_hiv_aids = HA1,
  used_computer_tablet = MT4,
  used_internet = MT9,
  owns_mobile_phone = MT11
)
```

```{r}
# Summarize missing values by column
summarize_missing <- sapply(female_df, function(x) sum(is.na(x)))
print(summarize_missing)
```

```{r}
# Recode the value 9 to NA for specified variables
female_df <- female_df %>%
  mutate(
    current_contraceptive_use = na_if(current_contraceptive_use, 9),
    used_internet = na_if(used_internet, 9),
    health_insurance = na_if(health_insurance, 9),
    education_level = na_if(education_level, 9),
    awareness_hiv_aids = na_if(awareness_hiv_aids, 9),
    used_computer_tablet = na_if(used_computer_tablet, 9),
    owns_mobile_phone = na_if(owns_mobile_phone, 9),
    marital_status = na_if(marital_status, 9)
  )

# Recode the value 6 to NA for 'ethnicity'
female_df$ethnicity <- na_if(female_df$ethnicity, 6)

# Recode the value 0 to NA for 'wealth_index'
female_df$wealth_index <- na_if(female_df$wealth_index, 0)
```

```{r}
# Recoding variables from 1 (Yes) and 2 (No) to 1 (Yes) and 0 (No)
female_df$health_insurance <- ifelse(female_df$health_insurance == 2, 0, female_df$health_insurance)
female_df$current_contraceptive_use <- ifelse(female_df$current_contraceptive_use == 2, 0, female_df$current_contraceptive_use)
female_df$awareness_hiv_aids <- ifelse(female_df$awareness_hiv_aids == 2, 0, female_df$awareness_hiv_aids)
female_df$used_computer_tablet <- ifelse(female_df$used_computer_tablet == 2, 0, female_df$used_computer_tablet)
female_df$owns_mobile_phone <- ifelse(female_df$owns_mobile_phone == 2, 0, female_df$owns_mobile_phone)
female_df$used_internet <- ifelse(female_df$used_internet == 2, 0, female_df$used_internet)
```

```{r}
# View the changes to ensure NA substitution has been correctly applied
summary(female_df)
```

### Creating New Variable `Access to Media`

```{r}
# Combine individual variables for access to the internet, phone, and computer into a single variable
# This new variable "access_to_media" will have a value of 1 if the respondent has access to any of these media sources, and 0 if not
# This provides a more comprehensive measure of media access
female_df <- female_df %>%
  mutate(access_to_media = ifelse(used_computer_tablet == 1 | used_internet == 1 | owns_mobile_phone == 1, 1, 0))
```

```{r}
# In later analysis, use access_to_media instead of the 3 separate variables
female_df <- female_df %>%
  dplyr::select(-used_computer_tablet, -used_internet, -owns_mobile_phone)
```


```{r}
# Removing rows where 'age_first_marriage' is NA in female_df
# This is because these values represent people who are not yet married, which is not the focus of the study
married_df <- female_df[!is.na(female_df$age_first_marriage), ]
summary(married_df)
```


```{r}
# Exporting female_df to a CSV file in the current working directory
#write.csv(married_df, "married_df.csv", row.names = FALSE)
```


### Distributions of Data

```{r}
# Histogram for 'Age at First Marriage'
afm_hist <- ggplot(married_df, aes(x = age_first_marriage)) +
  geom_histogram(fill = "#F7C0C8", color = "black") +
  theme_minimal() +
  ggtitle("Histogram of Age at First Marriage") +
  xlab("Age at First Marriage") +
  ylab("Frequency")

print(afm_hist)
```


### Handling Missing Data

```{r}
# Before that, let's double check the count of missing values by column
count_missing <- sapply(married_df, function(x) sum(is.na(x)))
print(count_missing)
```

```{r}
# Make a copy of female_df for imputation
imputed_df <- married_df
```

```{r}
# Define a function to calculate mode for categorical variables
getMode <- function(v) {
  # The mode is the value that appears most frequently in the data
  uniqv <- unique(na.omit(v))  # Omit NA values and get unique values
  uniqv[which.max(tabulate(match(v, uniqv)))]  # Return the value with the highest frequency
}
```

```{r}
# For 'ethnicity', an ordinal variable with predefined categories, it makes sense to impute missing values with the mode.
mode_ethnicity <- getMode(imputed_df$ethnicity)
imputed_df <- mutate(imputed_df, ethnicity = ifelse(is.na(ethnicity), mode_ethnicity, ethnicity))
mode_area <- getMode(imputed_df$area)
imputed_df <- mutate(imputed_df, area = ifelse(is.na(area), mode_area, area))
mode_region <- getMode(imputed_df$region)
imputed_df <- mutate(imputed_df, region = ifelse(is.na(region), mode_region, region))
mode_marital_status <- getMode(imputed_df$marital_status)
imputed_df <- mutate(imputed_df, marital_status = ifelse(is.na(marital_status), mode_marital_status, marital_status))


# Binary variables like 'current_contraceptive_use', 'health_insurance','awareness_hiv_aids', 'used_internet', 'used_computer_tablet', 'owns_mobile_phone', and 'access_to_media'
# should be imputed with the mode since it represents the most frequent category (either 0 or 1).

# Calculate the mode for each binary variable
mode_current_contraceptive_use <- getMode(imputed_df$current_contraceptive_use)
mode_health_insurance <- getMode(imputed_df$health_insurance)
mode_awareness_hiv_aids <- getMode(imputed_df$awareness_hiv_aids)
mode_access_to_media <- getMode(imputed_df$access_to_media)

# Impute missing values for binary variables
imputed_df <- mutate(imputed_df,
  current_contraceptive_use = ifelse(is.na(current_contraceptive_use), mode_current_contraceptive_use, current_contraceptive_use),
  health_insurance = ifelse(is.na(health_insurance), mode_health_insurance, health_insurance),
  awareness_hiv_aids = ifelse(is.na(awareness_hiv_aids), mode_awareness_hiv_aids, awareness_hiv_aids),
  access_to_media = ifelse(is.na(access_to_media), mode_access_to_media, access_to_media)
)

# 'education_level' is an ordinal variable where the median could be a more suitable measure of central tendency than the mode.
# However, given the categorical nature of the levels (e.g., "Primary", "Secondary"), using the mode may still be appropriate.
mode_education_level <- getMode(imputed_df$education_level)
imputed_df <- mutate(imputed_df, education_level = ifelse(is.na(education_level), mode_education_level, education_level))

# 'wealth_index' is an ordinal variable where the median could be a more suitable measure of central tendency than the mode.
# However, given the categorical nature of the levels (e.g., "Poorest", "Poor",...), using the mode may still be appropriate.
mode_wealth_index <- getMode(imputed_df$wealth_index)
imputed_df <- mutate(imputed_df, wealth_index = ifelse(is.na(wealth_index), mode_wealth_index, wealth_index))
```


```{r}
# Check the resulting dataset to confirm changes
summary(imputed_df)
```

```{r}
# After imputation, let's check for missing values
count_imputation <- sapply(imputed_df, function(x) sum(is.na(x)))
print(count_imputation)
```

### Visualization Comparing Before vs. After Imputation

```{r}
# Histogram for 'ethnicity' before imputation
ethnic <- ggplot(married_df, aes(x = ethnicity)) + 
  geom_histogram(fill = "#F7C0C8", color = "black", bins = 30) +
  theme_light() +
  ggtitle("Before Imputation") +
  xlab("Ethnicity") +
  ylab("Frequency")

# Histogram for 'ethnicity' after imputation
ethnic_imputed <- ggplot(imputed_df, aes(x = ethnicity)) + 
  geom_histogram(fill = "#E83853", color = "black", bins = 30) +
  theme_light() +
  ggtitle("After Imputation") +
  xlab("Ethnicity") +
  ylab("Frequency")

# Arrange the two plots side by side
grid.arrange(ethnic, ethnic_imputed, ncol = 2)
```

```{r}
# Histogram for 'education_level' before imputation
educ <- ggplot(married_df, aes(x = education_level)) + 
  geom_histogram(fill = "#F7C0C8", color = "black", bins = 30) +
  theme_light() +
  ggtitle("Before Imputation") +
  xlab("Education Level") +
  ylab("Frequency")

# Histogram for 'education_level' after imputation
educ_imputed <- ggplot(imputed_df, aes(x = education_level)) + 
  geom_histogram(fill = "#E83853", color = "black", bins = 30) +
  theme_light() +
  ggtitle("After Imputation") +
  xlab("Education Level") +
  ylab("Frequency")

# Arrange the two plots side by side
grid.arrange(educ, educ_imputed, ncol = 2)
```


```{r}
# Arrange the two plots side by side and capture the layout as a grob
#combined_plots <- arrangeGrob(afm_0, afm_imputed, ncol = 2)

# Now, use ggsave to save the combined plot
#ggsave("combined_age_first_marriage.png", plot = combined_plots, width = 10, height = 5)
```

### Creating Binary Variables for Child Marriage Under 18 and 16

```{r}
# Convert "age at first marriage" into a binary variable to indicate child marriage
# Child marriage is defined as marriage before the age of 18
# The new binary variable "child_marriage" will have a value of 1 if the marriage occurred before age 18, and 0 otherwise
imputed_df <- imputed_df %>%
  mutate(child_marriage = ifelse(age_first_marriage < 18, 1, 0))
```

```{r}
# Create a binary variable for child marriage under 16
# The new variable "child_marriage_u16" will have a value of 1 if the marriage occurred before age 16, and 0 otherwise
imputed_df <- imputed_df %>%
  mutate(child_marriage_u16 = ifelse(age_first_marriage < 16, 1, 0))
```

```{r}
# Move "child_marriage" and "child_marriage_u16" to the front of the dataframe
imputed_df <- imputed_df %>%
  dplyr::select(child_marriage, child_marriage_u16, everything())
```

```{r}
# Exporting female_df to a CSV file in the current working directory
#write.csv(imputed_df, "imputed_df.csv", row.names = FALSE)
```


### EDA

#### Vietnam Map with Regions and Their Percentages of Child Marriage (under 18)

```{r}
# Aggregate the data by region to get the total number of child marriages under 18 per region
region_counts <- aggregate(child_marriage ~ region, data = imputed_df, FUN = sum)

# Calculate the total number of child marriages under 18 in the dataset
total_child_marriages <- sum(region_counts$child_marriage)

# Calculate the percentage for each region
region_counts$married_u18_perc_of_total <- (region_counts$child_marriage / total_child_marriages) * 100
```

```{r}
# Mapping region numbers to names
region_names <- c("Red River Delta", "Northern Midlands And Mountain", 
                  "North Central And Central Coastal", "Central Highlands", 
                  "South East", "Mekong River Delta")
names(region_counts)[1] <- "region_name"
region_counts$region_name <- factor(region_counts$region_name, levels = 1:6, labels = region_names)

# Display the final data frame
print(region_counts)
```


```{r}
# Updated mapping including all provinces and cities in the Red River Delta
province_to_region <- data.frame(
  NAME_1 = c(
    'Bắc Ninh', 'Hà Nam', 'Hà Nội', 'Hải Dương', 'Hải Phòng', 'Hoà Bình', 'Hưng Yên', 'Nam Định', 'Ninh Bình', 'Thái Bình', 'Vĩnh Phúc', # Red River Delta 11
    
    'Bắc Giang', 'Bắc Kạn', 'Cao Bằng', 'Hà Giang', 'Lạng Sơn', 'Lào Cai', 'Phú Thọ', 'Quảng Ninh', 'Thái Nguyên', 'Tuyên Quang', 'Yên Bái', 'Điện Biên', 'Lai Châu', 'Sơn La', # Northern Midlands And Mountain 14
    
    'Bình Định', 'Bình Thuận', 'Khánh Hòa', 'Ninh Thuận', 'Phú Yên', 'Quảng Nam', 'Quảng Ngãi', 'Thừa Thiên Huế', 'Đà Nẵng', 'Hà Tĩnh', 'Nghệ An', 'Quảng Bình', 'Quảng Trị', 'Thanh Hóa', # North Central And Central Coastal 14
    
    'Đắk Lắk', 'Đắk Nông', 'Gia Lai', 'Kon Tum', 'Lâm Đồng', # Central Highlands 5
    
    'Bà Rịa - Vũng Tàu', 'Bình Dương', 'Bình Phước', 'Đồng Nai', 'Hồ Chí Minh', 'Tây Ninh', # South East 6
    
    'An Giang', 'Bạc Liêu', 'Bến Tre', 'Cà Mau', 'Cần Thơ', 'Đồng Tháp', 'Hậu Giang', 'Kiên Giang', 'Long An', 'Sóc Trăng', 'Tiền Giang', 'Trà Vinh', 'Vĩnh Long' # Mekong River Delta 13
  ),
  Region = c(
    rep('Red River Delta', 11), 
    rep('Northern Midlands And Mountain', 14), 
    rep('North Central And Central Coastal', 14), 
    rep('Central Highlands', 5), 
    rep('South East', 6), 
    rep('Mekong River Delta', 13)
  )
)

```


```{r}
# Check the mapping
#print(province_to_region)
```


```{r message=FALSE, warning=FALSE}
# Read shapefile
vietnam_shape <- st_read('/Users/hollyduong/Desktop/DA 401/ChildMarriageInVietnam/gadm41_VNM_shp')

# Join the shapefile with the province-to-region mapping
vietnam_shape_with_region <- vietnam_shape %>%
  left_join(province_to_region, by = "NAME_1")

# Aggregate the shapefile data by region
vietnam_regions <- vietnam_shape_with_region %>%
  group_by(Region) %>%
  summarise(geometry = st_union(geometry), .groups = 'drop')
```


```{r}
# Join the aggregated shapefile data with the child marriage data
vietnam_map_data <- vietnam_regions %>%
  left_join(region_counts, by = c("Region" = "region_name"))

# Define colors with your specific choices
colors_ordered <- setNames(c("#B20033", "#CD0A25", "#E83853", "#EF7D8D", "#F7C0C8", "#FBE1E5"),
                           c("Northern Midlands And Mountain", "Central Highlands", 
                             "Mekong River Delta", "North Central And Central Coastal", 
                             "South East", "Red River Delta"))

# Plot
mapvn <- ggplot(data = vietnam_map_data) +
  geom_sf(aes(fill = factor(Region, levels = names(colors_ordered))), color = NA) +
  geom_sf_text(aes(label = sprintf("%.1f%%", married_u18_perc_of_total)), size = 4, hjust = 0.5, vjust = 0.5) +
  scale_fill_manual(values = colors_ordered, name = "Region") +
  labs(title = "Regional Contribution of Female Child Marriage Rates to Total Rates in Vietnam") +
  theme_void() +
  theme(legend.position = "right")

print(mapvn)
```

```{r}
#ggsave("mapvn.png", plot = mapvn, width = 8, height = 6, dpi = 300)
```

#### Marital Status Among Female Respondants by Region in Vietnam

```{r}
total_child_marriages_u18 <- sum(imputed_df$child_marriage == 1)

# Aggregate counts for each category by region
counts_df <- aggregate(cbind(married_u18 = imputed_df$child_marriage == 1, 
                             married_u16 = imputed_df$child_marriage_u16 == 1) ~ region, 
                       data = imputed_df, 
                       FUN = sum)

# Convert counts to percentages based on total child marriages under 18
counts_df$married_u18 <- (counts_df$married_u18 / total_child_marriages_u18) * 100
counts_df$married_u16 <- (counts_df$married_u16 / total_child_marriages_u18) * 100
```


```{r}
p <- ggplot(counts_df, aes(x = factor(region))) +
  geom_bar(aes(y = married_u18, fill = "Married before 18 years old"), stat = "identity", width = 0.5) +
  geom_bar(aes(y = married_u16, fill = "Married before 16 years old"), stat = "identity", width = 0.5) +
  
  # Adding text labels for married_u18
  geom_text(aes(y = married_u18, label = sprintf("%.1f%%", married_u18)), 
            position = position_stack(vjust = 1.03), 
            size = 4, color = "black") +
  # Adding text labels for married_u16
  geom_text(aes(y = married_u16, label = sprintf("%.1f%%", married_u16)), 
            position = position_stack(vjust = 1.05), 
            size = 4, color = "black") +
  
  scale_fill_manual(values = c("Married before 18 years old" = "#EF7D8D", 
                               "Married before 16 years old" = "#B20016"),
                    name = "Marital Status") +
  labs(x = "Region", y = "Percentage", title = "Marital Status Among Female Respondents by Region", element_text(size = 14)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 18),
        plot.margin = margin(t = 10, r = 10, b = 10, l = 10, unit = "mm"), 
        axis.text.x = element_text(size = 14, angle = 45, hjust = 1), 
        legend.position = c(0.85,0.8),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 14)) +
  scale_x_discrete(labels = c("1" = "Red River Delta", "2" = "Northern Midlands And Mountain", 
                              "3" = "North Central And Central Coastal", "4" = "Central Highlands", 
                              "5" = "South East", "6" = "Mekong River Delta"))

print(p)
```

```{r}
#ggsave("marital_status.png", plot = p, width = 10, height = 5)
```


### Preparing for Regression Analysis

#### Correlation Matrix

```{r}
# Calculate correlation matrix
cor_matrix <- cor(imputed_df %>% select_if(is.numeric), use = "complete.obs")

# Melt the correlation matrix
melted_cor_matrix <- melt(cor_matrix)
```

```{r}
# Generate an interactive heatmap
corr_matrix <- ggplot(melted_cor_matrix, aes(Var1, Var2, fill = value)) +
    geom_tile() +
    scale_fill_gradientn(
        colours = c("deepskyblue", "white", "#CD0A25"),
        values = scales::rescale(c(-1, 0, 1)),
        limits = c(-1, 1),
        name="Pearson\nCorrelation"
    ) +
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    xlab("") + 
    ylab("") +
    ggtitle("Correlation Matrix") 

# Convert ggplot object to plotly for interactivity
ggplotly(corr_matrix)
```

```{r}
#write.csv(imputed_df, "imputed_df.csv", row.names = FALSE)
```


#### Converting Categorical and Binary Variables to Factors

```{r}
# Convert nominal and ordinal variables to factors
imputed_df$area <- as.factor(imputed_df$area)
imputed_df$region <- as.factor(imputed_df$region)
imputed_df$education_level <- factor(imputed_df$education_level, ordered = FALSE)
imputed_df$ethnicity <- as.factor(imputed_df$ethnicity)
imputed_df$wealth_index <- factor(imputed_df$wealth_index, ordered = FALSE)

# Binary variables are already in the correct format and can be used as is
```



### Logistic Regression Analysis

### Model 1 - Base Logistic Regression Model

```{r}
# Baseline model for reference
baseline_model <- glm(child_marriage ~ area + education_level + wealth_index + health_insurance + current_contraceptive_use + awareness_hiv_aids + access_to_media, family = binomial(), data = imputed_df)

# Summarize the baseline model
summary(baseline_model)
```

### Model 2 - Adding Fixed Effects to Base Model

```{r}
# Enhanced model with additional fixed effects
enhanced_model <- glm(child_marriage ~ area + region + education_level + ethnicity + wealth_index + health_insurance + current_contraceptive_use + awareness_hiv_aids + access_to_media, 
                      family = binomial(), 
                      data = imputed_df)

# Summarize the new model with FEs
summary(enhanced_model)
```

### Model 3 - Model with Interaction Terms (area:education_level)

```{r}
# Adding the interaction term between area and education level
model_interaction <- update(enhanced_model, . ~ . + area:education_level)
summary(model_interaction)
```


### Logistic Regression Models Comparison (Odd Ratios and 95% Confidence Intervals)

```{r}
# Function to add significance asterisks
add_asterisks <- function(p_value) {
  if (is.na(p_value)) {
    return(NA)
  } else if (p_value < 0.001) {
    return("***")
  } else if (p_value < 0.01) {
    return("**")
  } else if (p_value < 0.05) {
    return("*")
  } else {
    return("")
  }
}
```

```{r}
# Function to format confidence intervals as a string
format_ci <- function(lower, upper) {
  paste0("(", round(lower, 2), ", ", round(upper, 2), ")")
}
```

```{r}
# Tidy the baseline model with confidence intervals
tidy_baseline <- tidy(baseline_model, conf.int = TRUE, exponentiate = TRUE)

# Tidy the enhanced model with confidence intervals
tidy_enhanced <- tidy(enhanced_model, conf.int = TRUE, exponentiate = TRUE)

# Tidy the interaction model with confidence intervals
tidy_interaction <- tidy(model_interaction, conf.int = TRUE, exponentiate = TRUE)
```

```{r}
# Apply the function to each model's p.value
tidy_baseline$asterisks <- sapply(tidy_baseline$p.value, add_asterisks)
tidy_enhanced$asterisks <- sapply(tidy_enhanced$p.value, add_asterisks)
tidy_interaction$asterisks <- sapply(tidy_interaction$p.value, add_asterisks)
```

```{r}
# Create OR strings with asterisks and format CIs as a string
tidy_baseline <- tidy_baseline %>%
  mutate(
    OR = ifelse(is.na(estimate), NA, paste0(round(estimate, 2), asterisks)),
    CI = ifelse(is.na(conf.low) | is.na(conf.high), NA, format_ci(conf.low, conf.high))
  )

tidy_enhanced <- tidy_enhanced %>%
  mutate(
    OR = ifelse(is.na(estimate), NA, paste0(round(estimate, 2), asterisks)),
    CI = ifelse(is.na(conf.low) | is.na(conf.high), NA, format_ci(conf.low, conf.high))
  )

tidy_interaction <- tidy_interaction %>%
  mutate(
    OR = ifelse(is.na(estimate), NA, paste0(round(estimate, 2), asterisks)),
    CI = ifelse(is.na(conf.low) | is.na(conf.high), NA, format_ci(conf.low, conf.high))
  )
```

```{r}
# Add a 'Model' column to each tidied dataframe
tidy_baseline <- tidy_baseline %>% mutate(Model = "Model_1")
tidy_enhanced <- tidy_enhanced %>% mutate(Model = "Model_2")
tidy_interaction <- tidy_interaction %>% mutate(Model = "Model_3")

# Combine and pivot the dataframes
combined_results <- bind_rows(
  tidy_baseline %>% dplyr::select(term, OR, CI, Model),
  tidy_enhanced %>% dplyr::select(term, OR, CI, Model),
  tidy_interaction %>% dplyr::select(term, OR, CI, Model)
) %>%
  pivot_wider(names_from = Model, values_from = c(OR, CI))
```

```{r}
# Replace NAs with "—"
combined_results[is.na(combined_results)] <- "—"
```

```{r}
# Reordering columns to have OR and CI next to each other for each model
combined_results <- combined_results %>%
  dplyr::select(term, 
         OR_Model_1, CI_Model_1, 
         OR_Model_2, CI_Model_2, 
         OR_Model_3, CI_Model_3)
```

```{r}
# Print the final combined table
print(combined_results)
```


### Models Validation

#### Likelihood Ratio Test (LRT)

```{r}
# Model 1 vs. Model 2
anova(baseline_model, enhanced_model, test="Chisq")
```

The Model 2 provides a significantly better fit to the data compared to the Model 1, as indicated by the large decrease in residual deviance and the very small p-value (< 2.2e-16).

```{r}
# Model 1 vs. Model 3
anova(baseline_model, model_interaction, test="Chisq")
```

The Model 3 provides a significantly better fit to the data compared to the Model 1, as indicated by the large decrease in residual deviance and the very small p-value (< 2.2e-16).

```{r}
# Model 2 vs. Model 3
anova(enhanced_model, model_interaction, test="Chisq")
```

The p-value is not significant.


#### ROC Curve and AUC

```{r message=FALSE, warning=FALSE}
invisible(plot(roc(imputed_df$child_marriage,
                   fitted(baseline_model)),
               col = "red",
               main = "ROC Curve: \nModel 1 (red) vs. Model 2 (green) vs. Model 3 (blue)"))

invisible(plot(roc(imputed_df$child_marriage,
                   fitted(enhanced_model)),
               col = "green",
               add = T))

invisible(plot(roc(imputed_df$child_marriage,
                   fitted(model_interaction)),
               print.auc = T,
               col = "blue",
               add = T))

```

```{r message=FALSE}
# For each model
roc_response_model_1 <- roc(imputed_df$child_marriage, fitted(baseline_model))
auc_model_1 <- auc(roc_response_model_1)

roc_response_model_2 <- roc(imputed_df$child_marriage, fitted(enhanced_model))
auc_model_2 <- auc(roc_response_model_2)

roc_response_model_3 <- roc(imputed_df$child_marriage, fitted(model_interaction))
auc_model_3 <- auc(roc_response_model_3)

# Compare AUC values
print(paste("AUC Model 1:", auc_model_1))
print(paste("AUC Model 2:", auc_model_2))
print(paste("AUC Model 3:", auc_model_3))
```

1. Model 1 (AUC: 0.7723):

An AUC of 0.5 represents a model with no discriminative ability (akin to random guessing), while an AUC of 1.0 represents perfect discrimination. So, my model is performing substantially better than random guessing.

The AUC of 0.7 indicates that the Model 1 has good discriminative ability. In other words, it is capable of distinguishing between cases and controls with a high degree of accuracy.

2. Model 2 (AUC: 0.7868):

This model shows a slight improvement in AUC over the Model 1. The increase suggests that the additional variables (or adjustments) I made in the Model 2 contribute positively to its ability to differentiate between cases and controls.
The difference in AUC between the Model 1 and Model 2, while modest, is still meaningful, especially in practical, real-world contexts.

3. Model 3 (AUC: 0.7876):

The Model 3 shows a very slight improvement in AUC over the Model 2. This indicates that adding interaction terms provides a marginal improvement in the model's discriminatory power.
However, the improvement is very minimal, which aligns with my earlier findings that the interaction terms did not significantly improve the model fit.

4. Overall:

All models demonstrate good ability to distinguish between cases and controls. An AUC greater than 0.7 is generally considered acceptable, and my models are above 0.7 and close to 0.8.
The Model 2 and Model 3 only show marginal improvements in AUC compared to the Model 1. This suggests that while the additional complexity (more variables and interaction terms) does contribute slightly to model performance, the gains are not substantial.


#### Residuals Plot

```{r message=FALSE, warning=FALSE}
binnedplot(fitted(model_interaction),
           residuals(model_interaction, type = "response"),
           nclass = NULL,
           xlab = "Expected Values",
           ylab = "Average Residuals",
           main = "Binned Residual Plot",
           cex.pts = 1,
           col.int = "gray")
```

#### Hosmer-Lemeshow Test

```{r}
# 1. Hosmer-Lemeshow Test for the Model 1
hoslem.test(baseline_model$y, fitted(baseline_model), g=10)

# 2. Hosmer-Lemeshow Test for the Model 2 (Baseline + Fixed Effects)
hoslem.test(enhanced_model$y, fitted(enhanced_model), g=10)

# 3. Hosmer-Lemeshow Test for the Model 3 (Baseline + Fixed Effects + Interaction Terms)
hoslem.test(model_interaction$y, fitted(model_interaction), g=10)
```

A large p-value (>0.05) indicates a good fit, meaning that there's no significant difference between the observed and predicted values. Through each model, the p-value increases which suggests that our decision to include fixed effects and interaction terms are significant.


#### Accessing Multicollinearity (VIF)

```{r}
# VIFs check (A VIF value > 5 indicates high multicollinearity)
# Base model
model_1_vif <- vif(baseline_model)
print(model_1_vif)

# Enhanced model
model_2_vif <- vif(enhanced_model)
print(model_2_vif)

# Interacton model
model_3_vif <- vif(model_interaction)
print(model_3_vif)
```

- Model 1 has the least concern with multicollinearity.
- Model 2 introduces `region` and `ethnicity`, with a mild increase in multicollinearity, but not at alarming levels.
- Model 3 exhibits more noticeable multicollinearity, particularly with the `area` variable and its interaction with `education_level`; their VIF values are significantly higher (around 3.57, 2.3, and 2.37, respectively). Although these values are below 5, the increase is substantial compared to the previous models and could start to affect the reliability and interpretability of the regression coefficients for these variables.
- High multicollinearity in models, especially those with interaction terms, doesn't invalidate the model but does complicate the interpretation of specific coefficients.


#### Assessing AIC and BIC

```{r}
# Create a data frame to hold AIC and BIC values
aic_bic_comparison <- data.frame(
    Model = c("Model 1", "Model 2", "Model 3"),
    AIC = c(AIC(baseline_model), AIC(enhanced_model), AIC(model_interaction)),
    BIC = c(BIC(baseline_model), BIC(enhanced_model), BIC(model_interaction))
)

# Print the table
print(aic_bic_comparison)
```

Model 2 has the lowest AIC and BIC values of all, indicating that it might be the best model among the three in terms of balancing fit and complexity.



## Conservative Approach: Redo Analysis with Actual Data (No Imputation)

### Data Prep

#### Recreate DataFrames with Removed NAs

```{r}
# No imputation -- Remove rows with any missing data from 'female_df'
og_df <- na.omit(married_df)
print(og_df)
```

#### Recreate Necessary Variables

```{r}
# Convert "age at first marriage" into a binary variable to indicate child marriage
# Child marriage is defined as marriage before the age of 18
# The new binary variable "child_marriage" will have a value of 1 if the marriage occurred before age 18, and 0 otherwise
og_df <- og_df %>%
  mutate(child_marriage = ifelse(age_first_marriage < 18, 1, 0))
```

```{r}
# Create a binary variable for child marriage under 16
# The new variable "child_marriage_u16" will have a value of 1 if the marriage occurred before age 16, and 0 otherwise
og_df <- og_df %>%
  mutate(child_marriage_u16 = ifelse(age_first_marriage < 16, 1, 0))
```

```{r}
# Move "child_marriage" and "child_marriage_u16" to the front of the dataframe
og_df <- og_df %>%
  dplyr::select(child_marriage, child_marriage_u16, everything())
```

#### Converting Categorical and Binary Variables to Factors

```{r}
# Convert nominal and ordinal variables to factors
og_df$area <- as.factor(og_df$area)
og_df$region <- as.factor(og_df$region)
og_df$education_level <- factor(og_df$education_level, ordered = FALSE)
og_df$ethnicity <- as.factor(og_df$ethnicity)
og_df$wealth_index <- factor(og_df$wealth_index, ordered = FALSE)

# Binary variables are already in the correct format and can be used as is
```

```{r}
#write.csv(og_df, "og_df.csv", row.names = FALSE)
```


### Recreate Logistic Regression Models Without Imputed Data

#### Model 1: Base Model

```{r}
# Base model
base_model_no_impute <- glm(child_marriage ~ area + education_level + wealth_index + health_insurance + current_contraceptive_use + awareness_hiv_aids + access_to_media, family = binomial(), data = og_df)

# Summarize the base model
summary(base_model_no_impute)
```

#### Model 2: Base Model with Fixed Effects

```{r}
# Enhanced model with additional fixed effects
enhanced_model_no_impute <- glm(child_marriage ~ area + region + education_level + ethnicity + wealth_index + health_insurance + current_contraceptive_use + awareness_hiv_aids + access_to_media, family = binomial(), data = og_df)

# Summarize the new model with FEs
summary(enhanced_model_no_impute)
```


#### Model 3: Base Model with Fixed Effects and Interaction Terms

```{r}
# Adding the interaction term between  area and education level
interaction_model_no_impute <- update(enhanced_model_no_impute, . ~ . + area:education_level)
summary(interaction_model_no_impute)
```


#### Models With vs Without Imputed Data

```{r}
# Extracting data from the models
extract_model_data <- function(model) {
  model_summary <- summary(model)
  coeffs <- model_summary$coefficients
  data.frame(
    Term = rownames(coeffs),
    Estimate = sprintf("%.3f", coeffs[, "Estimate"]),
    pValue = ifelse(coeffs[, "Pr(>|z|)"] < 0.001, 
                    format(coeffs[, "Pr(>|z|)"], scientific = TRUE),
                    sprintf("%.3f", coeffs[, "Pr(>|z|)"])),
    Significance = sapply(coeffs[, "Pr(>|z|)"], add_asterisks)
  )
}
```

#### Model 1: Base Model

```{r}
# Applying the function to each model
base_impute_data <- extract_model_data(baseline_model)
base_no_impute_data <- extract_model_data(base_model_no_impute)

# Combine the data for comparison
base_comparison_data <- merge(base_impute_data, base_no_impute_data, by = "Term", suffixes = c("_Impute", "_NoImpute"), sort = FALSE)

# View the comparison
print(base_comparison_data)
```

- Most variables show consistent results in terms of significance and direction of effect across both models. This suggests that the imputation process has not drastically altered the relationships between these predictors and the outcome variable.
- Education and Wealth Index variables are consistent, robust and significant predictors of the outcome across both models.
- Significance of Area: This variable's significance suggests geographical variation is an important factor, but its influence is reduced in the non-imputed model.
- The overall patterns suggest that while some predictors are consistently influential, others are sensitive to how missing data is handled.

#### Model 2: Base Model with Fixed Effects

```{r}
# Applying the function to each model
enhanced_impute_data <- extract_model_data(enhanced_model)
enhanced_no_impute_data <- extract_model_data(enhanced_model_no_impute)

# Combine the data for comparison
enhanced_comparison_data <- merge(enhanced_impute_data, enhanced_no_impute_data, by = "Term", suffixes = c("_Impute", "_NoImpute"), sort = FALSE)

# View the comparison
print(enhanced_comparison_data)
```

- Consistent findings across the imputed and non-imputed models in certain variables (e.g., education levels 3, 4, 5, and ethnicity4) suggest that imputation does not radically alter these relationships. This consistency can indicate that imputation is not introducing significant bias for these variables.
- Imputation might reveal statistically significant relationships that are not evident in the non-imputed data (e.g., area2, region2, region4, awareness of HIV/AIDS). 
- For some variables (e.g., wealth_index categories), the significance and estimate values change between the models. 
- The loss of significance in many variables in the non-imputed model suggests that the missing data may not be random and could be related to the variables' influence on the outcome.

#### Model 3: Base Model with Fixed Effects and Interaction Terms

```{r}
# Applying the function to each model
interaction_impute_data <- extract_model_data(model_interaction)
interaction_no_impute_data <- extract_model_data(interaction_model_no_impute)

# Combine the data for comparison
interaction_comparison_data <- merge(interaction_impute_data, interaction_no_impute_data, by = "Term", suffixes = c("_Impute", "_NoImpute"), sort = FALSE)

# View the comparison
print(interaction_comparison_data)
```

- Consistency in the significance of variables like education levels 3, 4, and 5, and ethnicity4 across both models suggests that imputation is not drastically changing these relationships.
- The imputed model shows significant effects in variables (like region2, region4, ethnicity3, and interaction terms) that are not significant in the non-imputed model.
- The differing levels of significance for variables like wealth index categories and awareness of HIV/AIDS between models raise questions about how missing data impacts these variables.
- The loss of significance in many variables in the non-imputed model suggests that the missing data may not be random and could be related to the variables' influence on the outcome.


### Logistic Regression Models Comparison (Odd Ratios and 95% Confidence Intervals)

```{r}
# Tidy the baseline model with confidence intervals
tidy_base_no_impute <- tidy(base_model_no_impute, conf.int = TRUE, exponentiate = TRUE)

# Tidy the enhanced model with confidence intervals
tidy_enhanced_no_impute <- tidy(enhanced_model_no_impute, conf.int = TRUE, exponentiate = TRUE)

# Tidy the interaction model with confidence intervals
tidy_interaction_no_impute <- tidy(interaction_model_no_impute, conf.int = TRUE, exponentiate = TRUE)
```

```{r}
# Apply the function to each model's p.value
tidy_base_no_impute$asterisks <- sapply(tidy_base_no_impute$p.value, add_asterisks)
tidy_enhanced_no_impute$asterisks <- sapply(tidy_enhanced_no_impute$p.value, add_asterisks)
tidy_interaction_no_impute$asterisks <- sapply(tidy_interaction_no_impute$p.value, add_asterisks)
```

```{r}
# Create OR strings with asterisks and format CIs as a string
tidy_base_no_impute <- tidy_base_no_impute %>%
  mutate(
    OR = ifelse(is.na(estimate), NA, paste0(round(estimate, 2), asterisks)),
    CI = ifelse(is.na(conf.low) | is.na(conf.high), NA, format_ci(conf.low, conf.high))
  )

tidy_enhanced_no_impute <- tidy_enhanced_no_impute %>%
  mutate(
    OR = ifelse(is.na(estimate), NA, paste0(round(estimate, 2), asterisks)),
    CI = ifelse(is.na(conf.low) | is.na(conf.high), NA, format_ci(conf.low, conf.high))
  )

tidy_interaction_no_impute <- tidy_interaction_no_impute %>%
  mutate(
    OR = ifelse(is.na(estimate), NA, paste0(round(estimate, 2), asterisks)),
    CI = ifelse(is.na(conf.low) | is.na(conf.high), NA, format_ci(conf.low, conf.high))
  )
```

```{r}
# Add a 'Model' column to each tidied dataframe
tidy_base_no_impute <- tidy_base_no_impute %>% mutate(Model = "Model_1")
tidy_enhanced_no_impute <- tidy_enhanced_no_impute %>% mutate(Model = "Model_2")
tidy_interaction_no_impute <- tidy_interaction_no_impute %>% mutate(Model = "Model_3")

# Combine and pivot the dataframes
combined_no_impute <- bind_rows(
  tidy_base_no_impute %>% dplyr::select(term, OR, CI, Model),
  tidy_enhanced_no_impute %>% dplyr::select(term, OR, CI, Model),
  tidy_interaction_no_impute %>% dplyr::select(term, OR, CI, Model)
) %>%
  pivot_wider(names_from = Model, values_from = c(OR, CI))
```

```{r}
# Replace NAs with "—"
combined_no_impute[is.na(combined_no_impute)] <- "—"
```

```{r}
# Reordering columns to have OR and CI next to each other for each model
combined_no_impute <- combined_no_impute %>%
  dplyr::select(term, 
         OR_Model_1, CI_Model_1, 
         OR_Model_2, CI_Model_2, 
         OR_Model_3, CI_Model_3)
```

```{r}
# Print the final combined table
print(combined_no_impute)
```

1. `(Intercept)` Odds Ratios and Significance:

- In Model 1, the odds ratio for the intercept is 0.88, which indicates a slight negative association, but it's not statistically significant, as indicated by the absence of asterisks.
- In Model 2, the intercept has an OR of 0.19, which is highly significant (indicated by three asterisks) and suggests a much stronger negative association.
- Model 3 shows a similar pattern to Model 2, with a significant OR of 0.27, indicating a robust negative association.

2. `Area2`:

- This term has an OR of 1.3 in Model 1 and is statistically significant (denoted by two asterisks), suggesting a moderately positive association with the outcome.
- In Models 2 and 3, the ORs are 1.16 and 1.15, respectively, showing a slight positive association, but these are not statistically significant.

3. `Education Level` Categories:

- All education levels in Model 1 show a significant negative association (ORs less than 1, with three asterisks), meaning higher education levels are associated with lower odds of the outcome.
- In Models 2 and 3, education_level1 and education_level2 lose their statistical significance (ORs 1.15 and 1.09 in Model 2, and 1.14 and 1.08 in Model 3), indicating no strong association.
- However, education_level3, education_level4, and education_level5 maintain their strong, significant negative association in Models 2 and 3, as indicated by the ORs (0.49, 0.06, and 0.08) and the presence of three asterisks.

4. `Wealth Index` Categories:

- In Model 1, all wealth index categories show a significant negative association with the outcome, with ORs ranging from 0.36 to 0.54 and marked with three asterisks.
- The strength of this association decreases in Models 2 and 3, with ORs moving closer to 1 and varying levels of significance, showing that wealth's impact on the outcome is less pronounced or potentially confounded by other variables in these models.

5. `Health Insurance`:

- There is no statistically significant association between health insurance and the outcome in any model, as indicated by ORs around 1 and the absence of asterisks.

6. `Current Contraceptive Use`:

- Similar to health insurance, this variable shows no significant association with the outcome in any of the models.

7. `Awareness of HIV/AIDS`:
- In Model 1, there is a significant negative association (OR 0.62, three asterisks), but this association becomes non-significant in Models 2 and 3.

8. `Access to Media`:

This variable shows no consistent or significant association with the outcome across the three models.

9. `Region` Variables in Models 2 and 3:

- These variables are only included in the latter two models and show varying ORs, but none of them are statistically significant.

10. `Ethnicity` Categories in Models 2 and 3:

- Ethnicity shows significant positive associations in these models, particularly for `ethnicity4`, which has very high ORs and is highly significant.

11. Interaction Terms `ethnicity:access_to_media` in Model 3:

- The interaction terms involving ethnicity and access to media are introduced in Model 3, but none show statistical significance.


### Models Validation

#### Likelihood Ratio Test (LRT)

```{r}
# Model 1 vs. Model 2
anova(base_model_no_impute, enhanced_model_no_impute, test="Chisq")
```

The Model 2 provides a significantly better fit to the data compared to the Model 1, as indicated by the large decrease in residual deviance and the very small p-value (< 2.2e-16).


```{r}
# Model 1 vs. Model 3
anova(base_model_no_impute, interaction_model_no_impute, test="Chisq")
```

The Model 3 provides a significantly better fit to the data compared to the Model 1, as indicated by the large decrease in residual deviance and the very small p-value (< 2.2e-16).

```{r}
# Model 2 vs. Model 3
anova(enhanced_model_no_impute, interaction_model_no_impute, test="Chisq")
```

The residual deviance in Model 3 is slightly lower compared to Model 1, indicating a potential improvement in model fit but not significant.

The p-value of 0.5896 is not below the common significance level. This indicates that the addition of the interaction term `area:education_level` in Model 3 does not significantly improve the model fit compared to Model 2. In simpler terms, the interaction between `area` and `education_level` does not seem to have a significant effect on predicting child marriage.


#### ROC Curve and AUC

```{r message=FALSE, warning=FALSE}
invisible(plot(roc(og_df$child_marriage,
                   fitted(base_model_no_impute)),
               col = "red",
               main = "ROC Curve: \nModel 1 (red) vs. Model 2 (green) vs. Model 3 (blue)"))

invisible(plot(roc(og_df$child_marriage,
                   fitted(enhanced_model_no_impute)),
               col = "green",
               add = T))

invisible(plot(roc(og_df$child_marriage,
                   fitted(interaction_model_no_impute)),
               print.auc = T,
               col = "blue",
               add = T))

```

```{r message=FALSE}
# For each model
roc_model_1 <- roc(og_df$child_marriage, fitted(base_model_no_impute))
auc_mod_1 <- auc(roc_model_1)

roc_model_2 <- roc(og_df$child_marriage, fitted(enhanced_model_no_impute))
auc_mod_2 <- auc(roc_model_2)

roc_model_3 <- roc(og_df$child_marriage, fitted(interaction_model_no_impute))
auc_mod_3 <- auc(roc_model_3)

# Compare AUC values
print(paste("AUC Model 1:", auc_mod_1))
print(paste("AUC Model 2:", auc_mod_2))
print(paste("AUC Model 3:", auc_mod_3))
```

1. Model 1 (AUC: 0.7834):

An AUC of 0.5 represents a model with no discriminative ability (akin to random guessing), while an AUC of 1.0 represents perfect discrimination. So, my model is performing substantially better than random guessing.

The AUC of 0.7 indicates that the Model 1 has good discriminative ability. In other words, it is capable of distinguishing between cases and controls with a high degree of accuracy.

2. Model 2 (AUC: 0.7998):

This model shows a slight improvement in AUC over the Model 1. The increase suggests that the additional variables (or adjustments) I made in the Model 2 contribute positively to its ability to differentiate between cases and controls.
The difference in AUC between the Model 1 and Model 2, while modest, is still meaningful, especially in practical, real-world contexts.

3. Model 3 (AUC: 0.8000):

The Model 3 shows a very slight improvement in AUC over the Model 2. This indicates that adding interaction terms provides a marginal improvement in the model's discriminatory power.
However, the improvement is very minimal, which aligns with my earlier findings that the interaction terms did not significantly improve the model fit.

4. Overall:

All models demonstrate good ability to distinguish between cases and controls. An AUC greater than 0.7 is generally considered acceptable, and my models are above 0.7 and model 3 equals to 0.8.
The Model 2 and Model 3 only show marginal improvements in AUC compared to the Model 1. This suggests that while the additional complexity (more variables and interaction terms) does contribute slightly to model performance, the gains are not substantial.


#### Residuals Plot

```{r message=FALSE, warning=FALSE}
binnedplot(fitted(interaction_model_no_impute),
           residuals(interaction_model_no_impute, type = "response"),
           nclass = NULL,
           xlab = "Expected Values",
           ylab = "Average Residuals",
           main = "Binned Residual Plot",
           cex.pts = 1,
           col.int = "gray")
```

#### Hosmer-Lemeshow Test

```{r}
# 1. Hosmer-Lemeshow Test for the Model 1
hoslem.test(base_model_no_impute$y, fitted(base_model_no_impute), g=10)

# 2. Hosmer-Lemeshow Test for the Model 2 (Baseline + Fixed Effects)
hoslem.test(enhanced_model_no_impute$y, fitted(enhanced_model_no_impute), g=10)

# 3. Hosmer-Lemeshow Test for the Model 3 (Baseline + Fixed Effects + Interaction Terms)
hoslem.test(interaction_model_no_impute$y, fitted(interaction_model_no_impute), g=10)
```

A large p-value (>0.05) indicates a good fit, meaning that there's no significant difference between the observed and predicted values. Through each model, the p-value increases which suggests that our decision to include fixed effects and interaction terms are significant.

#### Accessing Multicollinearity (VIF)

```{r}
# VIFs check (A VIF value > 5 indicates high multicollinearity)
# Base model
model_1_vif <- vif(base_model_no_impute)
print(model_1_vif)

# Enhanced model
model_2_vif <- vif(enhanced_model_no_impute)
print(model_2_vif)

# Interacton model
model_3_vif <- vif(interaction_model_no_impute)
print(model_3_vif)
```

- Model 1 has the least concern with multicollinearity.
- Model 2 introduces `region` and `ethnicity`, with a mild increase in multicollinearity, but not at alarming levels.
- Model 3 exhibits more noticeable multicollinearity, particularly with `area`, `education_level` and its interaction with one another`; their VIF values are significantly higher (around 4.2, 2.17, and 2.25, respectively). Although these values are below 5, the increase is substantial compared to the previous models and could start to affect the reliability and interpretability of the regression coefficients for these variables.
- High multicollinearity in models, especially those with interaction terms, doesn't invalidate the model but does complicate the interpretation of specific coefficients.


#### Assessing AIC and BIC

```{r}
# Create a data frame to hold AIC and BIC values
aic_bic_comparison <- data.frame(
    Model = c("Model 1", "Model 2", "Model 3"),
    AIC = c(AIC(base_model_no_impute), AIC(enhanced_model_no_impute), AIC(interaction_model_no_impute)),
    BIC = c(BIC(base_model_no_impute), BIC(enhanced_model_no_impute), BIC(interaction_model_no_impute))
)

# Print the table
print(aic_bic_comparison)
```

Model 2 has the lowest AIC and BIC values of all, indicating that it might be the best model among the three in terms of balancing fit and complexity. 
